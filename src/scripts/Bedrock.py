import boto3
import json
from dotenv import load_dotenv
from typing import List, Optional
from typing_extensions import Literal
from pydantic import BaseModel

class Usage(BaseModel):
    input_tokens: int
    """The number of input tokens which were used."""

    output_tokens: int
    """The number of output tokens which were used."""


class ContentBlock(BaseModel):
    text: str

    type: Literal["text"]


class Message(BaseModel):
    id: str
    """Unique object identifier.

    The format and length of IDs may change over time.
    """

    content: List[ContentBlock]
    """Content generated by the model.

    This is an array of content blocks, each of which has a `type` that determines
    its shape. Currently, the only `type` in responses is `"text"`.

    Example:

    ```json
    [{ "type": "text", "text": "Hi, I'm Claude." }]
    ```

    If the request input `messages` ended with an `assistant` turn, then the
    response `content` will continue directly from that last turn. You can use this
    to constrain the model's output.

    For example, if the input `messages` were:

    ```json
    [
      {
        "role": "user",
        "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
      },
      { "role": "assistant", "content": "The best answer is (" }
    ]
    ```

    Then the response `content` might be:

    ```json
    [{ "type": "text", "text": "B)" }]
    ```
    """

    model: str
    """The model that handled the request."""

    role: Literal["assistant"]
    """Conversational role of the generated message.

    This will always be `"assistant"`.
    """

    stop_reason: Optional[Literal["end_turn", "max_tokens", "stop_sequence"]] = None
    """The reason that we stopped.

    This may be one the following values:

    - `"end_turn"`: the model reached a natural stopping point
    - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
    - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated

    Note that these values are different than those in `/v1/complete`, where
    `end_turn` and `stop_sequence` were not differentiated.

    In non-streaming mode this value is always non-null. In streaming mode, it is
    null in the `message_start` event and non-null otherwise.
    """

    stop_sequence: Optional[str] = None
    """Which custom stop sequence was generated, if any.

    This value will be a non-null string if one of your custom stop sequences was
    generated.
    """

    type: Literal["message"]
    """Object type.

    For Messages, this is always `"message"`.
    """

    usage: Usage
    """Billing and rate-limit usage.

    Anthropic's API bills and rate-limits by token counts, as tokens represent the
    underlying cost to our systems.

    Under the hood, the API transforms requests into a format suitable for the
    model. The model's output then goes through a parsing stage before becoming an
    API response. As a result, the token counts in `usage` will not match one-to-one
    with the exact visible content of an API request or response.

    For example, `output_tokens` will be non-zero, even for an empty string response
    from Claude.
    """



class BedrockClient:
    """
    A client for interacting with the Bedrock service.

    Args:
        profile_name (str): The name of the AWS profile to use for authentication.

    Attributes:
        client (botocore.client.BaseClient): The Bedrock runtime client.

    """

    def __init__(self, profile_name):
        load_dotenv()
        session = boto3.Session(profile_name=profile_name)
        self.client = session.client("bedrock-runtime")

    def create_claude_body(
        self,
        messages=[{"role": "user", "content": "Hello!"}],
        system="",
        token_count=150,
        temp=0,
        topP=1,
        topK=250,
        stop_sequence=["Human"],
    ):
        """
        Create the request body for the Claude model.

        Args:
            messages (list, optional): List of messages in the conversation. Each message should have a "role" and "content" field. Defaults to [{"role": "user", "content": "Hello!"}].
            system (str, optional): The system message. Defaults to "".
            token_count (int, optional): The maximum number of tokens in the response. Defaults to 150.
            temp (int, optional): The temperature for generating the response. Defaults to 0.
            topP (int, optional): The top-p value for generating the response. Defaults to 1.
            topK (int, optional): The top-k value for generating the response. Defaults to 250.
            stop_sequence (list, optional): List of stop sequences to end the response. Defaults to ["Human"].

        Returns:
            dict: The request body for the Claude model.

        """
        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "messages": messages,
            "max_tokens": token_count,
            "temperature": temp,
            "anthropic_version": "",
            "top_k": topK,
            "top_p": topP,
            "stop_sequences": stop_sequence,
            "system": system,
        }
        return body

    def get_claude_response(
        self,
        messages="",
        system="",
        token_count=4000,
        temp=0,
        topP=1,
        topK=250,
        stop_sequence=["Human:"],
        model_id="anthropic.claude-3-haiku-20240307-v1:0",
    ) -> Message:
        """
        Get the response from the Claude model.

        Args:
            messages (list, optional): List of messages in the conversation. Each message should have a "role" and "content" field. Defaults to "".
            system (str, optional): The system message. Defaults to "".
            token_count (int, optional): The maximum number of tokens in the response. Defaults to 250.
            temp (int, optional): The temperature for generating the response. Defaults to 0.
            topP (int, optional): The top-p value for generating the response. Defaults to 1.
            topK (int, optional): The top-k value for generating the response. Defaults to 250.
            stop_sequence (list, optional): List of stop sequences to end the response. Defaults to ["Human:"].
            model_id (str, optional): The ID of the Claude model. Defaults to "anthropic.claude-3-sonnet-20240229-v1:0".

        Returns:
            dict: The response from the Claude model.

        """
        body = self.create_claude_body(
            messages=messages,
            system=system,
            token_count=token_count,
            temp=temp,
            topP=topP,
            topK=topK,
            stop_sequence=stop_sequence,
        )
        response = self.client.invoke_model(modelId=model_id, body=json.dumps(body))
        final_response = Message.model_validate(response["body"].read().decode("utf-8"))
        return final_response
